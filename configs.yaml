---
# data:
qa_data: data/Train_qa_ans.json # data/SampleData_QA.json
risk_data: data/Train_risk_classification_ans.csv # data/SampleData_RiskClassification.csv
dev_qa_data: data/Develop_QA.json
dev_risk_data: data/Develop_risk_classification.csv

# device
device_id: 0 # cpu: -1, gpu: >= 0

# train
seed: 1126
epochs: 5
batch_size: 3
learning_rate: 0.0001
warmup_steps: 1000
val_size: 0.2

log_step: 1

# model
model_class: ClsAttention
model: Roberta
hidden_dim: 256
dropout: 0.2
warmup_epoch: 10
latent_mode: cls
activation: GELU
  
# data processing
dataset_class: qa_multiple_dataset
max_document_len: 512
max_question_len: 50
spkr: ['民眾', '個管師', '醫師', '護理師', '家屬', '藥師']
use_spkr_token: false
aug_mode: 'long'
span_size: 10
max_context_size: 50
t2s: null
medical_bert_dir: /home/leo/d/nlp/medical_bert
retrival_fn: self.retrival1

# qa models
min_sentence_len: 0
spkr_mode: 'token' # can be None, 'content', 'token'
ClsAttention:
  pretrained_cfg:
    pretrained: Roberta
    trainable_from: -1
    embedding_mode: pooler_output
    max_tokens: 10000
  hidden_size: 128
RetrivalBinary:
  pretrained_cfg:
    pretrained: Roberta
    trainable_from: 0
    embedding_mode: pooler_output
    max_tokens: 10000
RetrivalMultiple:
  pretrained_cfg:
    pretrained: Roberta
    trainable_from: 0
    embedding_mode: pooler_output
    max_tokens: 10000
